{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73b98f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91808\\Desktop\\Interview Preparation pdf\\NLP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\91808\\.cache\\kagglehub\\datasets\\lakshmi25npathi\\imdb-dataset-of-50k-movie-reviews\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c30350",
   "metadata": {},
   "source": [
    " Problem 1\n",
    "\n",
    " Apply all the preprocessing techniques that you think are necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a940b580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "#Load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your dataset file\n",
    "path = r\"C:\\Users\\91808\\.cache\\kagglehub\\datasets\\lakshmi25npathi\\imdb-dataset-of-50k-movie-reviews\\versions\\1\\IMDB Dataset.csv\"\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3969e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n",
      "None\n",
      "sentiment\n",
      "positive    25000\n",
      "negative    25000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Basic info and checks\n",
    "\n",
    "print(df.info())\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b28c89f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing steps (Full Pipeline) We’ll clean and prepare text for NLP models.\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45086e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91808\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91808\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\91808\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ec1c718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing function\n",
    "\n",
    "# Initialize lemmatizer \n",
    "lematizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42697c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get english stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    #Remove HTML Tags\n",
    "    text = re.sub(r'<.*?>','',text)\n",
    "\n",
    "    # Remove punctuation and special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenize (split text into words)\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Lemmatize words (convert running -> run)\n",
    "    words = [lematizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Join words back into the single strings\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dff54352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to dataset\n",
    "df['cleaned_review'] = df['review'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a268a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  \\\n",
      "0  One of the other reviewers has mentioned that ...   \n",
      "1  A wonderful little production. <br /><br />The...   \n",
      "2  I thought this was a wonderful way to spend ti...   \n",
      "\n",
      "                                      cleaned_review  \n",
      "0  one reviewer mentioned watching oz episode you...  \n",
      "1  wonderful little production filming technique ...  \n",
      "2  thought wonderful way spend time hot summer we...  \n"
     ]
    }
   ],
   "source": [
    "# Verify cleaned data\n",
    "print(df[['review','cleaned_review']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "210e2638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional): Save cleaned dataset\n",
    "df.to_csv(\"cleaned_imdb_dataset.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6120e50",
   "metadata": {},
   "source": [
    " Problem 2\n",
    "\n",
    " Find out the number of words in the entire corpus and also the total number of unique words(vocabulary) using just python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d33b1ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned dataset\n",
    " \n",
    "df = pd.read_csv(\"cleaned_imdb_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e5ef882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all reviews into one big text (the corpus)\n",
    "\n",
    "corpus = ' '.join(df['cleaned_review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aecce066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into individual words\n",
    "# Split the corpus into words\n",
    "words = corpus.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa4a8891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in corpus: 5930080\n",
      "Total number of unique words (vocabulary): 203439\n"
     ]
    }
   ],
   "source": [
    "# Count total and unique words\n",
    "\n",
    "# Total number of words into corpus\n",
    "total_words = len(words)\n",
    "\n",
    "# Total number of unique words(vocabulary)\n",
    "unique_words = len(set(words))\n",
    "\n",
    "print(\"Total number of words in corpus:\", total_words)\n",
    "print(\"Total number of unique words (vocabulary):\", unique_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1386eeac",
   "metadata": {},
   "source": [
    " Problem 3\n",
    "\n",
    " Apply One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d7f3609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies keras (tensorflow)\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f87f258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the text data\n",
    "\n",
    "# Example: use first 5 reviews for demonstration\n",
    "corpus = df['cleaned_review'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bdddf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vocabulary size\n",
    "#You must specify how many unique words (vocabulary size) you want to consider. For example, let’s use 10,000 (you can use the unique word count from Problem 2).\n",
    "\n",
    "vocab_size = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2699ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7349, 9392, 5197, 2416, 3725, 3254, 6190, 6392, 1616, 485, 3852, 7240, 5268, 572, 6820, 3725, 5968, 4605, 7894, 1052]\n"
     ]
    }
   ],
   "source": [
    "# Apply One-Hot Encoding\n",
    "\n",
    "# Encode each review into a list of integers\n",
    "onehot_repr = [one_hot(review, vocab_size) for review in corpus]\n",
    "\n",
    "# Display example\n",
    "print(onehot_repr[0][:20])  # first 20 encoded values of the first review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc6b5335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 100)\n"
     ]
    }
   ],
   "source": [
    "# (Optional): Padding sequences\n",
    "# Since reviews have different lengths, you often pad them to make them equal length for models.\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = 100  # you can adjust this\n",
    "padded_docs = pad_sequences(onehot_repr, padding='post', maxlen=max_length)\n",
    "\n",
    "print(padded_docs.shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ac8bc5",
   "metadata": {},
   "source": [
    " Problem 4\n",
    "\n",
    " Apply bag words and find the vocabulary also find the times each word has occured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18b1befe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words + Vocabulary + Word Counts\n",
    "# Import CountVectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c5d61cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vectorizer\n",
    "\n",
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d19d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform your cleaned text\n",
    "bow_matrix = cv.fit_transform(df['cleaned_review'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0db3397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 203415\n",
      "vocabulary: ['aa' 'aaa' 'aaaaaaaaaaaahhhhhhhhhhhhhh' ... 'zzzzzzzzzzzzz'\n",
      " 'zzzzzzzzzzzzzzzzzz' 'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz']\n"
     ]
    }
   ],
   "source": [
    "# Get the vocabulary (all unique words)\n",
    "\n",
    "vocab = cv.get_feature_names_out()\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "print(\"vocabulary:\",vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aaf745c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparse Matrix Shape: (50000, 20000)\n",
      "Vocabulary Size: 20000\n",
      "\n",
      "Top 20 most frequent words:\n",
      "movie: 99025\n",
      "film: 89809\n",
      "one: 52677\n",
      "like: 39790\n",
      "time: 29396\n",
      "good: 28615\n",
      "character: 27573\n",
      "get: 24435\n",
      "even: 24286\n",
      "story: 24229\n",
      "would: 24001\n",
      "make: 23564\n",
      "see: 23494\n",
      "really: 22900\n",
      "scene: 20706\n",
      "much: 18897\n",
      "well: 18629\n",
      "people: 17979\n",
      "great: 17803\n",
      "bad: 17673\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# ============================\n",
    "# MEMORY-SAFE CountVectorizer\n",
    "# ============================\n",
    "cv = CountVectorizer(\n",
    "    min_df=5,          # ignore words appearing in < 5 documents (remove rare words)\n",
    "    max_df=0.80,       # ignore words appearing in > 80% documents (too common)\n",
    "    max_features=20000 # limit vocabulary to top 20,000 words\n",
    ")\n",
    "\n",
    "# Fit and transform (keeps sparse matrix)\n",
    "bow_matrix = cv.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Print matrix shape\n",
    "print(\"Sparse Matrix Shape:\", bow_matrix.shape)\n",
    "\n",
    "# Vocabulary\n",
    "vocab = cv.get_feature_names_out()\n",
    "print(\"Vocabulary Size:\", len(vocab))\n",
    "\n",
    "# ============================\n",
    "# Memory-safe word frequency\n",
    "# ============================\n",
    "word_counts = np.asarray(bow_matrix.sum(axis=0)).ravel()\n",
    "\n",
    "# Word → count dictionary\n",
    "word_count_dict = dict(zip(vocab, word_counts))\n",
    "\n",
    "# Sort by count\n",
    "sorted_word_counts = sorted(word_count_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Show top 20 most frequent words\n",
    "print(\"\\nTop 20 most frequent words:\")\n",
    "for word, count in sorted_word_counts[:20]:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd412aa4",
   "metadata": {},
   "source": [
    " Problem 5\n",
    "\n",
    "Apply bag of bi-gram and bag of tri-gram and write down your observation about the dimensionality of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db72e029",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fc185740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Vocabulary Size: 203415\n"
     ]
    }
   ],
   "source": [
    "#------- UNIGRAM --------\n",
    "cv_uni = CountVectorizer()\n",
    "bow_uni = cv_uni.fit(df['cleaned_review'])\n",
    "vocab_uni = cv_uni.get_feature_names_out()\n",
    "print(\"Unigram Vocabulary Size:\",len(vocab_uni))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "95ac8f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Vocabulary Size: 3071115\n"
     ]
    }
   ],
   "source": [
    "# ---------- BIGRAM ----------\n",
    "cv_bi = CountVectorizer(ngram_range=(2,2))\n",
    "bow_bi = cv_bi.fit(df['cleaned_review'])\n",
    "vocab_bi = cv_bi.get_feature_names_out()\n",
    "print(\"Bigram Vocabulary Size:\", len(vocab_bi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb2ec3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Vocabulary Size: 5371719\n"
     ]
    }
   ],
   "source": [
    "# ---------- TRIGRAM ----------\n",
    "cv_tri = CountVectorizer(ngram_range=(3,3))\n",
    "bow_tri = cv_tri.fit(df['cleaned_review'])\n",
    "vocab_tri = cv_tri.get_feature_names_out()\n",
    "print(\"Trigram Vocabulary Size:\", len(vocab_tri))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7537935d",
   "metadata": {},
   "source": [
    "Problem 6\n",
    "\n",
    "Apply tf-idf and find out the idf scores of words, also find out the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "111cb13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 203415\n",
      "\n",
      "Sample Vocabulary: ['aa' 'aaa' 'aaaaaaaaaaaahhhhhhhhhhhhhh' 'aaaaaaaargh' 'aaaaaaah'\n",
      " 'aaaaaaahhhhhhggg' 'aaaaagh' 'aaaaah' 'aaaaargh'\n",
      " 'aaaaarrrrrrgggggghhhhhh' 'aaaaatchkah' 'aaaaaw' 'aaaahhhhhh'\n",
      " 'aaaahhhhhhh' 'aaaand' 'aaaarrgh' 'aaaawwwwww' 'aaaggghhhhhhh' 'aaaghi'\n",
      " 'aaah']\n",
      "\n",
      "IDF Table (first 20 rows):\n",
      "                          word  idf_score\n",
      "0                           aa   8.986585\n",
      "1                          aaa   9.254849\n",
      "2   aaaaaaaaaaaahhhhhhhhhhhhhh  11.126651\n",
      "3                  aaaaaaaargh  11.126651\n",
      "4                     aaaaaaah  11.126651\n",
      "5             aaaaaaahhhhhhggg  11.126651\n",
      "6                      aaaaagh  11.126651\n",
      "7                       aaaaah  11.126651\n",
      "8                     aaaaargh  11.126651\n",
      "9      aaaaarrrrrrgggggghhhhhh  11.126651\n",
      "10                 aaaaatchkah  11.126651\n",
      "11                      aaaaaw  11.126651\n",
      "12                  aaaahhhhhh  11.126651\n",
      "13                 aaaahhhhhhh  11.126651\n",
      "14                      aaaand  10.721186\n",
      "15                    aaaarrgh  11.126651\n",
      "16                  aaaawwwwww  11.126651\n",
      "17               aaaggghhhhhhh  11.126651\n",
      "18                      aaaghi  11.126651\n",
      "19                        aaah  10.721186\n",
      "\n",
      "Top 20 highest IDF words (most rare/unique):\n",
      "                                   word  idf_score\n",
      "203414  zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz  11.126651\n",
      "203413               zzzzzzzzzzzzzzzzzz  11.126651\n",
      "203412                    zzzzzzzzzzzzz  11.126651\n",
      "2            aaaaaaaaaaaahhhhhhhhhhhhhh  11.126651\n",
      "3                           aaaaaaaargh  11.126651\n",
      "203396                           zwicks  11.126651\n",
      "203392                        zwartboek  11.126651\n",
      "203390                      zvyagvatsev  11.126651\n",
      "203388                           zvezda  11.126651\n",
      "203387                         zuzzzuzz  11.126651\n",
      "203386                       zuwarriors  11.126651\n",
      "203385                            zuthe  11.126651\n",
      "203384                           zutaut  11.126651\n",
      "203383                         zurlinis  11.126651\n",
      "203382                          zurlini  11.126651\n",
      "36                                aaghh  11.126651\n",
      "35                                 aagh  11.126651\n",
      "32                          aadmittedly  11.126651\n",
      "31                                aadha  11.126651\n",
      "30                                 aada  11.126651\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Create TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Step 2: Fit the model on the cleaned text (DO NOT convert to array → saves memory)\n",
    "tfidf_matrix = tfidf.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Step 3: Vocabulary (all words)\n",
    "vocab = tfidf.get_feature_names_out()\n",
    "print(\"Vocabulary Size:\", len(vocab))\n",
    "print(\"\\nSample Vocabulary:\", vocab[:20])  # print first 20 words\n",
    "\n",
    "# Step 4: IDF scores of all words\n",
    "idf_scores = tfidf.idf_\n",
    "\n",
    "# Step 5: Create a table: word → idf\n",
    "idf_df = pd.DataFrame({\n",
    "    'word': vocab,\n",
    "    'idf_score': idf_scores\n",
    "})\n",
    "\n",
    "print(\"\\nIDF Table (first 20 rows):\")\n",
    "print(idf_df.head(20))\n",
    "\n",
    "# Step 6: Sort words by highest IDF (most unique words)\n",
    "idf_sorted = idf_df.sort_values(by='idf_score', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 highest IDF words (most rare/unique):\")\n",
    "print(idf_sorted.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5441df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
